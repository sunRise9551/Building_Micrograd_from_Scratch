# Building_Micrograd_from_Scratch

- The underlying principles of implementing neural network backpropagation, teachings sourced from Andrej.
- What we wil gain: An “under the hood” knowledge of deep learning: layer details, loss functions, optimization, etc.

# Project Overview
## Derivatives in Micrograd
- Understand the concept of derivative
- Calculate derivatives

## Value Object of Micrograd
- Implement basic operations: sum, substract, multiply, division
- Computre gradients
- Implement of Chain rule in backpropagation

## Neural Network Architecture
- Build Neurons, Layers, and Multi Layer Perceptron
- Create Activation functions
- Establish forward and backward function
- Construct parameters for gradient update


Youtube: https://www.youtube.com/watch?v=VMj-3S1tku0&t=1589s&ab_channel=AndrejKarpathy

GitHub: https://github.com/karpathy/micrograd

This Colab Notebook is simply a detailed compilation for my notes.
